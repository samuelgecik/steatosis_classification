{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steatosis Classification Model Fine-tuning\n",
    "\n",
    "This notebook implements the fine-tuning process with comprehensive metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from model import SteatosisModel, get_loss_fn\n",
    "from data import create_dataloaders\n",
    "from evaluation import MetricsCalculator\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "MODEL_PATH = 'Models/DenseNet121_processed.pt'\n",
    "DATA_DIR = 'DataSet'\n",
    "OUTPUT_DIR = 'training_output'\n",
    "LOG_DIR = 'logs'\n",
    "\n",
    "# Training parameters\n",
    "RUN_NAME = 'DenseNet121_processed-'+ datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "BATCH_SIZE = 16\n",
    "BINARY = True\n",
    "NUM_CLASSES = 2 if BINARY else 3\n",
    "\n",
    "# Create directories\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "Path(LOG_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2258, 0.8445])\n",
      "tensor([1.5000, 0.7500])\n",
      "tensor([1.2258, 0.8445])\n",
      "tensor([1.5000, 0.7500])\n",
      "Training samples: 7465\n",
      "Validation samples: 1083\n",
      "Training batches: 467\n",
      "Validation batches: 68\n"
     ]
    }
   ],
   "source": [
    "train_samples, val_samples = create_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=1,\n",
    "    binary=BINARY\n",
    ")\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    binary=BINARY\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pretrained weights\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and training components\n",
    "model = SteatosisModel(\n",
    "    pretrained_path=MODEL_PATH,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    freeze_layers=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = model.create_optimizer(\n",
    "    model.get_trainable_params(),\n",
    "    optimizer_type='adam',\n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = model.create_scheduler(optimizer, scheduler_type='plateau')\n",
    "loss_fn = get_loss_fn(NUM_CLASSES)\n",
    "metrics_calculator = MetricsCalculator(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, targets, metrics_calculator):\n",
    "    \"\"\"Compute metrics for a batch or epoch.\"\"\"\n",
    "    if outputs.dim() == 1:  # Binary\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "    else:  # Multi-class\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    metrics = metrics_calculator.compute_basic_metrics(targets, outputs)\n",
    "    metrics.update(\n",
    "        metrics_calculator.compute_roc_auc(\n",
    "            targets,\n",
    "            outputs,\n",
    "            multi_class=outputs.dim() > 1\n",
    "        )\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "# Reset BatchNorm statistics before validation\n",
    "def reset_bn_stats(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.BatchNorm2d):\n",
    "            m.reset_running_stats()\n",
    "            m.momentum = 0.1  # Default momentum\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, metrics_calculator, device):\n",
    "    \"\"\"Train for one epoch with detailed metric tracking.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # print(\".......................\")\n",
    "            # print(\"before modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "\n",
    "            \n",
    "            if output.shape[1] == 1:  # Binary classification\n",
    "                output = output.squeeze(1)\n",
    "                target = target.float()\n",
    "            \n",
    "            # print(\".......................\")\n",
    "            # print(\"after modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(output.detach())\n",
    "            all_targets.append(target)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "            })\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    outputs = torch.cat(all_outputs)\n",
    "    targets = torch.cat(all_targets)\n",
    "    epoch_metrics = compute_metrics(outputs, targets, metrics_calculator)\n",
    "    epoch_metrics['loss'] = total_loss / len(train_loader)\n",
    "    \n",
    "    return epoch_metrics\n",
    "\n",
    "def validate(model, val_loader, loss_fn, metrics_calculator, device):\n",
    "    \"\"\"Validate model and compute metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(val_loader, desc='Validation')):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # print(\".......................\")\n",
    "            # print(\"before modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "            if output.shape[1] == 1:  # Binary classification\n",
    "                output = output.squeeze(1)\n",
    "                target = target.float()\n",
    "            \n",
    "            # print(\".......................\")\n",
    "            # print(\"after modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "            loss = loss_fn(output, target)\n",
    "            # print(\"loss\")\n",
    "            # print(loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_outputs.append(output)\n",
    "            all_targets.append(target)\n",
    "        \n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    outputs = torch.cat(all_outputs)\n",
    "    targets = torch.cat(all_targets)\n",
    "    epoch_metrics = compute_metrics(outputs, targets, metrics_calculator)\n",
    "    print(\"epoch_metrics\")\n",
    "    epoch_metrics['loss'] = total_loss / len(val_loader)\n",
    "    \n",
    "    return epoch_metrics\n",
    "\n",
    "def log_metrics(metrics, phase, epoch, step=None):\n",
    "    \"\"\"Log metrics to JSON file.\"\"\"\n",
    "    log_entry = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'phase': phase,\n",
    "        'epoch': epoch,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    if step is not None:\n",
    "        log_entry['step'] = step\n",
    "    \n",
    "    log_file = Path(LOG_DIR) / f'training_metrics-{RUN_NAME}.json'\n",
    "    \n",
    "    # Load existing logs if any\n",
    "    if log_file.exists():\n",
    "        with open(log_file, 'r') as f:\n",
    "            logs = json.load(f)\n",
    "    else:\n",
    "        logs = []\n",
    "    \n",
    "    # Append new log entry\n",
    "    logs.append(log_entry)\n",
    "    \n",
    "    # Save updated logs\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(logs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Classifier Only (Phase 1)\n",
      "Learning rate: 0.001\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170da2f80d794781ab2d58623176a4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ab772ff01449588af43abbc35c1093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor(5174.7178, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6703, device='cuda:0')\n",
      "loss\n",
      "tensor(2667.6060, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6395, device='cuda:0')\n",
      "loss\n",
      "tensor(3200.9885, device='cuda:0')\n",
      "loss\n",
      "tensor(2597.7861, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6664, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6418, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6500, device='cuda:0')\n",
      "loss\n",
      "tensor(4538.1909, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6678, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6763, device='cuda:0')\n",
      "loss\n",
      "tensor(55297.2969, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6941, device='cuda:0')\n",
      "loss\n",
      "tensor(755.2360, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6566, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6305, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6685, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6298, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6762, device='cuda:0')\n",
      "loss\n",
      "tensor(79.6710, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5862, device='cuda:0')\n",
      "loss\n",
      "tensor(767.4988, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6375, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6781, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6396, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6749, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6739, device='cuda:0')\n",
      "loss\n",
      "tensor(8.5967, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6750, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6329, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6645, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6628, device='cuda:0')\n",
      "loss\n",
      "tensor(2.6882, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6757, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6295, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6217, device='cuda:0')\n",
      "loss\n",
      "tensor(30961.5059, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6696, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6683, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6249, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6880, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6436, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6774, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5924, device='cuda:0')\n",
      "loss\n",
      "tensor(29.8587, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6302, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6614, device='cuda:0')\n",
      "loss\n",
      "tensor(2041.1825, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6799, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6616, device='cuda:0')\n",
      "loss\n",
      "tensor(71.9604, device='cuda:0')\n",
      "loss\n",
      "tensor(6.6388, device='cuda:0')\n",
      "loss\n",
      "tensor(34958.1211, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6707, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5932, device='cuda:0')\n",
      "loss\n",
      "tensor(164875.8281, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6372, device='cuda:0')\n",
      "loss\n",
      "tensor(2770.7141, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6832, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6980, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6091, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6749, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6930, device='cuda:0')\n",
      "loss\n",
      "tensor(5.4480, device='cuda:0')\n",
      "loss\n",
      "tensor(26603.3223, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6459, device='cuda:0')\n",
      "loss\n",
      "tensor(202.3084, device='cuda:0')\n",
      "epoch_metrics\n",
      "Train Loss: 0.6912, F1: 0.5528, AUC: 0.5431\n",
      "Val Loss: 4965.4013, F1: 0.7652, AUC: 0.6737\n",
      "New best model saved! F1: 0.7652\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126a66cc02f2432a95e6555ecc335ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec3e683e495435bb29552ea096bd69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor(6.8308, device='cuda:0')\n",
      "loss\n",
      "tensor(1476.9443, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6788, device='cuda:0')\n",
      "loss\n",
      "tensor(187.5409, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5972, device='cuda:0')\n",
      "loss\n",
      "tensor(58305.9570, device='cuda:0')\n",
      "loss\n",
      "tensor(28.7566, device='cuda:0')\n",
      "loss\n",
      "tensor(1772.7063, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5911, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6766, device='cuda:0')\n",
      "loss\n",
      "tensor(79.7919, device='cuda:0')\n",
      "loss\n",
      "tensor(319.4043, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6744, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6226, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6912, device='cuda:0')\n",
      "loss\n",
      "tensor(1398.0076, device='cuda:0')\n",
      "loss\n",
      "tensor(25116.0859, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6606, device='cuda:0')\n",
      "loss\n",
      "tensor(17.6409, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6282, device='cuda:0')\n",
      "loss\n",
      "tensor(343337.5312, device='cuda:0')\n",
      "loss\n",
      "tensor(5406.6714, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6961, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5996, device='cuda:0')\n",
      "loss\n",
      "tensor(6838.4707, device='cuda:0')\n",
      "loss\n",
      "tensor(251.5798, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6391, device='cuda:0')\n",
      "loss\n",
      "tensor(588110.6250, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6739, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6444, device='cuda:0')\n",
      "loss\n",
      "tensor(560.9775, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6371, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6897, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6584, device='cuda:0')\n",
      "loss\n",
      "tensor(170.8975, device='cuda:0')\n",
      "loss\n",
      "tensor(2159.2656, device='cuda:0')\n",
      "loss\n",
      "tensor(246.8652, device='cuda:0')\n",
      "loss\n",
      "tensor(248.6043, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6455, device='cuda:0')\n",
      "loss\n",
      "tensor(1845.7079, device='cuda:0')\n",
      "loss\n",
      "tensor(4.9491, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6662, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6322, device='cuda:0')\n",
      "loss\n",
      "tensor(0.7004, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6366, device='cuda:0')\n",
      "loss\n",
      "tensor(5.4580, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6482, device='cuda:0')\n",
      "loss\n",
      "tensor(1057.4257, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6872, device='cuda:0')\n",
      "loss\n",
      "tensor(135896.4844, device='cuda:0')\n",
      "loss\n",
      "tensor(935.4529, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6132, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6734, device='cuda:0')\n",
      "loss\n",
      "tensor(2089.1545, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6325, device='cuda:0')\n",
      "loss\n",
      "tensor(600.0909, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6879, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5839, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6870, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6714, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6379, device='cuda:0')\n",
      "loss\n",
      "tensor(59590.9805, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6965, device='cuda:0')\n",
      "loss\n",
      "tensor(4082.4272, device='cuda:0')\n",
      "loss\n",
      "tensor(1233.4208, device='cuda:0')\n",
      "loss\n",
      "tensor(116.0295, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6147, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6519, device='cuda:0')\n",
      "epoch_metrics\n",
      "Train Loss: 0.6871, F1: 0.5066, AUC: 0.5806\n",
      "Val Loss: 18287.0818, F1: 0.6845, AUC: 0.6696\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c2023b39bd46f1aa78f6cb3da9b3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9416667480494644a152484835567ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor(0.6246, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6605, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6510, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6310, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6596, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6411, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6542, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6981, device='cuda:0')\n",
      "loss\n",
      "tensor(60.7694, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6273, device='cuda:0')\n",
      "loss\n",
      "tensor(0.7062, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6841, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6841, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6504, device='cuda:0')\n",
      "loss\n",
      "tensor(0.7213, device='cuda:0')\n",
      "loss\n",
      "tensor(0.7075, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6541, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6392, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6870, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6518, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6692, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6643, device='cuda:0')\n",
      "loss\n",
      "tensor(428.6830, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6738, device='cuda:0')\n",
      "loss\n",
      "tensor(49.3898, device='cuda:0')\n",
      "loss\n",
      "tensor(390.0576, device='cuda:0')\n",
      "loss\n",
      "tensor(374151.4375, device='cuda:0')\n",
      "loss\n",
      "tensor(772.2363, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6171, device='cuda:0')\n",
      "loss\n",
      "tensor(765.5848, device='cuda:0')\n",
      "loss\n",
      "tensor(0.7198, device='cuda:0')\n",
      "loss\n",
      "tensor(1442.8318, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6646, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6306, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6625, device='cuda:0')\n",
      "loss\n",
      "tensor(300866.0312, device='cuda:0')\n",
      "loss\n",
      "tensor(22031.9336, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6926, device='cuda:0')\n",
      "loss\n",
      "tensor(9.5282, device='cuda:0')\n",
      "loss\n",
      "tensor(186.1730, device='cuda:0')\n",
      "loss\n",
      "tensor(67884.2734, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6750, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6765, device='cuda:0')\n",
      "loss\n",
      "tensor(14635.2471, device='cuda:0')\n",
      "loss\n",
      "tensor(597.4980, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6587, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6308, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6760, device='cuda:0')\n",
      "loss\n",
      "tensor(40.1748, device='cuda:0')\n",
      "loss\n",
      "tensor(5.1436, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6680, device='cuda:0')\n",
      "loss\n",
      "tensor(24869.8926, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6691, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6926, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6617, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6849, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6749, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6411, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6651, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6986, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5739, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6565, device='cuda:0')\n",
      "loss\n",
      "tensor(5807.9858, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6009, device='cuda:0')\n",
      "loss\n",
      "tensor(0.7264, device='cuda:0')\n",
      "loss\n",
      "tensor(0.5910, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6634, device='cuda:0')\n",
      "loss\n",
      "tensor(0.6724, device='cuda:0')\n",
      "epoch_metrics\n",
      "Train Loss: 0.6854, F1: 0.5546, AUC: 0.5842\n",
      "Val Loss: 11985.6964, F1: 0.6639, AUC: 0.6962\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77a2e3d588d436cb58398b70444180b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m train_metrics = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_calculator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Log training metrics\u001b[39;00m\n\u001b[32m     36\u001b[39m log_metrics(train_metrics, \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, epoch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, loss_fn, metrics_calculator, device)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# print(\".......................\")\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# print(\"after modification\")\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# print(output.shape)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# print(target)\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# print(\".......................\")\u001b[39;00m\n\u001b[32m     58\u001b[39m loss = loss_fn(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     61\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/steatosis/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/steatosis/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/steatosis/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training phases configuration\n",
    "phases = [\n",
    "    {'name': 'Classifier Only', 'epochs': 10, 'blocks': None, 'lr': 1e-3},\n",
    "    {'name': 'Partial Unfreeze', 'epochs': 15, 'blocks': 2, 'lr': 1e-4},\n",
    "    {'name': 'Full Fine-tuning', 'epochs': 20, 'blocks': None, 'lr': 1e-5}\n",
    "]\n",
    "\n",
    "# Training history\n",
    "history = defaultdict(list)\n",
    "\n",
    "# Best model tracking\n",
    "best_metric = 0.0\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 5\n",
    "\n",
    "try:\n",
    "    # Training loop\n",
    "    for phase_idx, phase in enumerate(phases):\n",
    "        print(f\"\\nStarting {phase['name']} (Phase {phase_idx + 1})\")\n",
    "        print(f\"Learning rate: {phase['lr']}\")\n",
    "        \n",
    "        # Update model for this phase\n",
    "        model.unfreeze_layers(phase['blocks'])\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = phase['lr']\n",
    "        \n",
    "        for epoch in range(phase['epochs']):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{phase['epochs']}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = train_epoch(\n",
    "                model, train_loader, optimizer, loss_fn, metrics_calculator, device\n",
    "            )\n",
    "            \n",
    "            # Log training metrics\n",
    "            log_metrics(train_metrics, 'train', epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = validate(\n",
    "                model, val_loader, loss_fn, metrics_calculator, device\n",
    "            )\n",
    "            \n",
    "            # Log validation metrics\n",
    "            log_metrics(val_metrics, 'val', epoch)\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_metrics['f1_score'])\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Update history\n",
    "            for k, v in train_metrics.items():\n",
    "                history[f'train_{k}'].append(v)\n",
    "            for k, v in val_metrics.items():\n",
    "                history[f'val_{k}'].append(v)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(\n",
    "                f\"Train Loss: {train_metrics['loss']:.4f}, \"\n",
    "                f\"F1: {train_metrics['f1_score']:.4f}, \"\n",
    "                f\"AUC: {train_metrics['roc_auc']:.4f}\\n\"\n",
    "                f\"Val Loss: {val_metrics['loss']:.4f}, \"\n",
    "                f\"F1: {val_metrics['f1_score']:.4f}, \"\n",
    "                f\"AUC: {val_metrics['roc_auc']:.4f}\"\n",
    "            )\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['f1_score'] > best_metric:\n",
    "                best_metric = val_metrics['f1_score']\n",
    "                model.save(f\"{OUTPUT_DIR}/best_model.pt\")\n",
    "                patience_counter = 0\n",
    "                print(f\"New best model saved! F1: {best_metric:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {patience_counter} epochs without improvement\")\n",
    "                break\n",
    "        \n",
    "        # Save phase checkpoint\n",
    "        model.save(f\"{OUTPUT_DIR}/phase_{phase_idx + 1}_model.pt\")\n",
    "        \n",
    "    # Save final history\n",
    "    with open(f\"{OUTPUT_DIR}/training_history.json\", 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    metrics = [\n",
    "        ('loss', 'Loss'),\n",
    "        ('f1_score', 'F1 Score'),\n",
    "        ('roc_auc', 'ROC AUC'),\n",
    "        ('sensitivity', 'Sensitivity'),\n",
    "        ('specificity', 'Specificity')\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(metrics):\n",
    "        if idx < len(axes):\n",
    "            train_key = f'train_{metric}'\n",
    "            val_key = f'val_{metric}'\n",
    "            \n",
    "            if train_key in history and val_key in history:\n",
    "                axes[idx].plot(history[train_key], label='Train')\n",
    "                axes[idx].plot(history[val_key], label='Validation')\n",
    "                axes[idx].set_title(title)\n",
    "                axes[idx].set_xlabel('Epoch')\n",
    "                axes[idx].grid(True)\n",
    "                axes[idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/training_metrics.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_metrics(history)\n",
    "\n",
    "# Load and display detailed metrics\n",
    "with open(f\"{LOG_DIR}/training_metrics.json\", 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df = pd.json_normalize(logs)\n",
    "print(\"\\nFinal training metrics:\")\n",
    "print(df[df['phase'] == 'train'].tail())\n",
    "print(\"\\nFinal validation metrics:\")\n",
    "print(df[df['phase'] == 'val'].tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

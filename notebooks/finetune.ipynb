{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steatosis Classification Model Fine-tuning\n",
    "\n",
    "This notebook implements the fine-tuning process with comprehensive metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from src.model import SteatosisModel, get_loss_fn\n",
    "from src.data import create_dataloaders\n",
    "from src.evaluation import MetricsCalculator\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "MODEL_PATH = '../Models/DenseNet121_processed.pt'\n",
    "DATA_DIR = '../DataSet'\n",
    "OUTPUT_DIR = '../training_output'\n",
    "LOG_DIR = '../logs'\n",
    "\n",
    "# Training parameters\n",
    "RUN_NAME = 'DenseNet121_processed-'+ datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "BATCH_SIZE = 16\n",
    "BINARY = True\n",
    "NUM_CLASSES = 2 if BINARY else 3\n",
    "\n",
    "# Create directories\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "Path(LOG_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n",
    "print(f\"GPU device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2258, 0.8445])\n",
      "tensor([1.5000, 0.7500])\n",
      "tensor([1.2258, 0.8445])\n",
      "tensor([1.5000, 0.7500])\n",
      "Training samples: 7465\n",
      "Validation samples: 1083\n",
      "Training batches: 467\n",
      "Validation batches: 68\n"
     ]
    }
   ],
   "source": [
    "train_samples, val_samples = create_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=1,\n",
    "    binary=BINARY\n",
    ")\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    binary=BINARY\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pretrained weights\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and training components\n",
    "model = SteatosisModel(\n",
    "    pretrained_path=MODEL_PATH,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    freeze_layers=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = model.create_optimizer(\n",
    "    model.get_trainable_params(),\n",
    "    optimizer_type='adam',\n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = model.create_scheduler(optimizer, scheduler_type='plateau')\n",
    "loss_fn = get_loss_fn(NUM_CLASSES)\n",
    "metrics_calculator = MetricsCalculator(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, targets, metrics_calculator):\n",
    "    \"\"\"Compute metrics for a batch or epoch.\"\"\"\n",
    "    if outputs.dim() == 1:  # Binary\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "    else:  # Multi-class\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    metrics = metrics_calculator.compute_basic_metrics(targets, outputs)\n",
    "    metrics.update(\n",
    "        metrics_calculator.compute_roc_auc(\n",
    "            targets,\n",
    "            outputs,\n",
    "            multi_class=outputs.dim() > 1\n",
    "        )\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "# Reset BatchNorm statistics before validation\n",
    "def reset_bn_stats(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.BatchNorm2d):\n",
    "            m.reset_running_stats()\n",
    "            m.momentum = 0.1  # Default momentum\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, metrics_calculator, device):\n",
    "    \"\"\"Train for one epoch with detailed metric tracking.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with tqdm(train_loader, desc='Training') as pbar:\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # print(\".......................\")\n",
    "            # print(\"before modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "\n",
    "            \n",
    "            if output.shape[1] == 1:  # Binary classification\n",
    "                output = output.squeeze(1)\n",
    "                target = target.float()\n",
    "            \n",
    "            # print(\".......................\")\n",
    "            # print(\"after modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(output.detach())\n",
    "            all_targets.append(target)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "            })\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    outputs = torch.cat(all_outputs)\n",
    "    targets = torch.cat(all_targets)\n",
    "    epoch_metrics = compute_metrics(outputs, targets, metrics_calculator)\n",
    "    epoch_metrics['loss'] = total_loss / len(train_loader)\n",
    "    \n",
    "    return epoch_metrics\n",
    "\n",
    "def validate(model, val_loader, loss_fn, metrics_calculator, device):\n",
    "    \"\"\"Validate model and compute metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(val_loader, desc='Validation')):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # print(\".......................\")\n",
    "            # print(\"before modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "            if output.shape[1] == 1:  # Binary classification\n",
    "                output = output.squeeze(1)\n",
    "                target = target.float()\n",
    "            \n",
    "            # print(\".......................\")\n",
    "            # print(\"after modification\")\n",
    "            # print(output.shape)\n",
    "            # print(output)\n",
    "            # print(target.shape)\n",
    "            # print(target)\n",
    "            # print(\".......................\")\n",
    "            loss = loss_fn(output, target)\n",
    "            # print(\"loss\")\n",
    "            # print(loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_outputs.append(output)\n",
    "            all_targets.append(target)\n",
    "        \n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    outputs = torch.cat(all_outputs)\n",
    "    targets = torch.cat(all_targets)\n",
    "    epoch_metrics = compute_metrics(outputs, targets, metrics_calculator)\n",
    "    print(\"epoch_metrics\")\n",
    "    epoch_metrics['loss'] = total_loss / len(val_loader)\n",
    "    \n",
    "    return epoch_metrics\n",
    "\n",
    "def log_metrics(metrics, phase, epoch, step=None):\n",
    "    \"\"\"Log metrics to JSON file.\"\"\"\n",
    "    log_entry = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'phase': phase,\n",
    "        'epoch': epoch,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    if step is not None:\n",
    "        log_entry['step'] = step\n",
    "    \n",
    "    log_file = Path(LOG_DIR) / f'training_metrics-{RUN_NAME}.json'\n",
    "    \n",
    "    # Load existing logs if any\n",
    "    if log_file.exists():\n",
    "        with open(log_file, 'r') as f:\n",
    "            logs = json.load(f)\n",
    "    else:\n",
    "        logs = []\n",
    "    \n",
    "    # Append new log entry\n",
    "    logs.append(log_entry)\n",
    "    \n",
    "    # Save updated logs\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(logs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Classifier Only (Phase 1)\n",
      "Learning rate: 0.001\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b3986361ae435882aad0ddf836ca7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db15c05c65c84fbc9ff93ae3d8a501bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6906, F1: 0.5186, AUC: 0.5502\n",
      "Val Loss: 8572.0540, F1: 0.7471, AUC: 0.6904\n",
      "New best model saved! F1: 0.7471\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d054efe070477383909e6a3ae479c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6486baed5cd34f3fb9ca6af87311ed3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6871, F1: 0.5246, AUC: 0.5782\n",
      "Val Loss: 1761.8949, F1: 0.6519, AUC: 0.6900\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7026620eb645d1bc41144d03ad9c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4f4c8b303a41c3b44394d5144de6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6844, F1: 0.5627, AUC: 0.5926\n",
      "Val Loss: 9877.7178, F1: 0.6782, AUC: 0.6804\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef414d96d1d40129a30f48412637c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13f5952a4674bdb95ce48f9249ac735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6840, F1: 0.5897, AUC: 0.5907\n",
      "Val Loss: 14744.3886, F1: 0.7867, AUC: 0.6750\n",
      "New best model saved! F1: 0.7867\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf5a2bb362848d7b943c0d2adde027b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de0d4aaf5e849409802bd8ad2703405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6815, F1: 0.5907, AUC: 0.6039\n",
      "Val Loss: 12101.1860, F1: 0.6778, AUC: 0.6854\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be531a153ea14b44a1d87e8ce48cfe7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afe12f4bb944cc8af76307302993462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6800, F1: 0.5846, AUC: 0.6022\n",
      "Val Loss: 7237.1933, F1: 0.7590, AUC: 0.6924\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f909ac7592d24132bbe74fe83ec01c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdf477225e249d9b9f07568ed3718f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6813, F1: 0.5944, AUC: 0.6027\n",
      "Val Loss: 17875.7390, F1: 0.6842, AUC: 0.6877\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b8b2bb5d1146d7b38385d39c471596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b224fcde194a70acdc7bebbb7991f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6780, F1: 0.6113, AUC: 0.6146\n",
      "Val Loss: 9251.3136, F1: 0.7333, AUC: 0.6931\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fbba40d9404e04ab281e03d1cec1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bff446c1d414fc6aec077ab01b8b0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6776, F1: 0.6241, AUC: 0.6194\n",
      "Val Loss: 27484.7007, F1: 0.7179, AUC: 0.6709\n",
      "Early stopping triggered after 5 epochs without improvement\n",
      "\n",
      "Starting Partial Unfreeze (Phase 2)\n",
      "Learning rate: 0.0001\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cd6d8e374d4c9e87ca4e2efe95d251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ea2c23b2274a4a9731e5b03fb56d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6772, F1: 0.6120, AUC: 0.6207\n",
      "Val Loss: 11615.5697, F1: 0.7286, AUC: 0.6928\n",
      "Early stopping triggered after 6 epochs without improvement\n",
      "\n",
      "Starting Full Fine-tuning (Phase 3)\n",
      "Learning rate: 1e-05\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ec0757e41b4f8da6621891e0c03895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5e7ed318674ecaa0010bba2deb5e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_metrics\n",
      "Train Loss: 0.6785, F1: 0.6068, AUC: 0.6096\n",
      "Val Loss: 13609.4104, F1: 0.7112, AUC: 0.6646\n",
      "Early stopping triggered after 7 epochs without improvement\n"
     ]
    }
   ],
   "source": [
    "# Training phases configuration\n",
    "phases = [\n",
    "    {'name': 'Classifier Only', 'epochs': 10, 'blocks': None, 'lr': 1e-3},\n",
    "    {'name': 'Partial Unfreeze', 'epochs': 15, 'blocks': 2, 'lr': 1e-4},\n",
    "    {'name': 'Full Fine-tuning', 'epochs': 20, 'blocks': None, 'lr': 1e-5}\n",
    "]\n",
    "\n",
    "# Training history\n",
    "history = defaultdict(list)\n",
    "\n",
    "# Best model tracking\n",
    "best_metric = 0.0\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 5\n",
    "\n",
    "try:\n",
    "    # Training loop\n",
    "    for phase_idx, phase in enumerate(phases):\n",
    "        print(f\"\\nStarting {phase['name']} (Phase {phase_idx + 1})\")\n",
    "        print(f\"Learning rate: {phase['lr']}\")\n",
    "        \n",
    "        # Update model for this phase\n",
    "        model.unfreeze_layers(phase['blocks'])\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = phase['lr']\n",
    "        \n",
    "        for epoch in range(phase['epochs']):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{phase['epochs']}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = train_epoch(\n",
    "                model, train_loader, optimizer, loss_fn, metrics_calculator, device\n",
    "            )\n",
    "            \n",
    "            # Log training metrics\n",
    "            log_metrics(train_metrics, 'train', epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = validate(\n",
    "                model, val_loader, loss_fn, metrics_calculator, device\n",
    "            )\n",
    "            \n",
    "            # Log validation metrics\n",
    "            log_metrics(val_metrics, 'val', epoch)\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_metrics['f1_score'])\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Update history\n",
    "            for k, v in train_metrics.items():\n",
    "                history[f'train_{k}'].append(v)\n",
    "            for k, v in val_metrics.items():\n",
    "                history[f'val_{k}'].append(v)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(\n",
    "                f\"Train Loss: {train_metrics['loss']:.4f}, \"\n",
    "                f\"F1: {train_metrics['f1_score']:.4f}, \"\n",
    "                f\"AUC: {train_metrics['roc_auc']:.4f}\\n\"\n",
    "                f\"Val Loss: {val_metrics['loss']:.4f}, \"\n",
    "                f\"F1: {val_metrics['f1_score']:.4f}, \"\n",
    "                f\"AUC: {val_metrics['roc_auc']:.4f}\"\n",
    "            )\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['f1_score'] > best_metric:\n",
    "                best_metric = val_metrics['f1_score']\n",
    "                model.save(f\"{OUTPUT_DIR}/best_model.pt\")\n",
    "                patience_counter = 0\n",
    "                print(f\"New best model saved! F1: {best_metric:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {patience_counter} epochs without improvement\")\n",
    "                patience_counter = 0\n",
    "                break\n",
    "        \n",
    "        # Save phase checkpoint\n",
    "        model.save(f\"{OUTPUT_DIR}/phase_{phase_idx + 1}_model.pt\")\n",
    "        \n",
    "    # Save final history\n",
    "    with open(f\"{OUTPUT_DIR}/training_history.json\", 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    metrics = [\n",
    "        ('loss', 'Loss'),\n",
    "        ('f1_score', 'F1 Score'),\n",
    "        ('roc_auc', 'ROC AUC'),\n",
    "        ('sensitivity', 'Sensitivity'),\n",
    "        ('specificity', 'Specificity')\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(metrics):\n",
    "        if idx < len(axes):\n",
    "            train_key = f'train_{metric}'\n",
    "            val_key = f'val_{metric}'\n",
    "            \n",
    "            if train_key in history and val_key in history:\n",
    "                axes[idx].plot(history[train_key], label='Train')\n",
    "                axes[idx].plot(history[val_key], label='Validation')\n",
    "                axes[idx].set_title(title)\n",
    "                axes[idx].set_xlabel('Epoch')\n",
    "                axes[idx].grid(True)\n",
    "                axes[idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/training_metrics.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_metrics(history)\n",
    "\n",
    "# Load and display detailed metrics\n",
    "with open(f\"{LOG_DIR}/training_metrics.json\", 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df = pd.json_normalize(logs)\n",
    "print(\"\\nFinal training metrics:\")\n",
    "print(df[df['phase'] == 'train'].tail())\n",
    "print(\"\\nFinal validation metrics:\")\n",
    "print(df[df['phase'] == 'val'].tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
